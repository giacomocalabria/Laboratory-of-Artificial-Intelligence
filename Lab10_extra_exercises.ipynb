{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twbLWgpO7YQH"
      },
      "source": [
        "## EXERCISE 1: What to do at the airport?\n",
        "\n",
        "You are travelling and have some time to kill at the aiport. There are three things you could spend your time doing:\n",
        "  \n",
        "1) You could have a coffee.\n",
        "\n",
        "This has a probability of $0.8$ of giving you time to relax with a tasty beverage, and a utility of $10$.\n",
        "It also has a probability of $0.2$ of providing you with a nasty cup from over-roasted beans that annoys you,\n",
        "and outcome with a utility of $-5$.\n",
        "\n",
        "2) You could shop for clothes.\n",
        "\n",
        "This has a probability of $0.1$ that you will find a great outfit at a good price, utility $20$. However, it\n",
        "has a probability of $0.9$ that you end up wasting money on over-priced junk, utility $-10$.\n",
        "\n",
        "3) You could have a bite to eat.\n",
        "\n",
        "This has a probability of $0.8$ that you find something rather mediocre that prevents you from being too hungry\n",
        "during your flight, utility $2$, and a probability of $0.2$ that you find something filling and tasty, utility $5$.\n",
        "\n",
        "> __QUESTION 1(a):__ What should you do if you take the principle of maximum expected utility to be your decision criterion?\n",
        "\n",
        "> __QUESTION 1(b):__ What should you do if you take the principle of maximax decision criterion to be your decision criterion?\n",
        "\n",
        "> __QUESTION 1(c):__ What should you do if you take the principle of maximin decision criterion to be your decision criterion?\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfHPaBECSq2y",
        "outputId": "00b6dd1d-228e-4339-86b0-6e9bac8d7778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EU by outcome = [ 8. -1.]\n",
            "EU(coffee) =  7.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "coffee_outcomes = [\"relax\",\"annoys\"]\n",
        "u_coffee_outcomes = np.array([10, -5])\n",
        "p_coffee_outcomes_coffee = np.array([0.8, 0.2])\n",
        "\n",
        "# The weighted utility ofeach outcome is each to compute by pairwise multiplication\n",
        "eu_coffee_outcome = u_coffee_outcomes * p_coffee_outcomes_coffee\n",
        "print('EU by outcome =', eu_coffee_outcome)\n",
        "\n",
        "# Summing the weighted utilities gets us the expected utility\n",
        "eu_coffee = np.sum(eu_coffee_outcome)\n",
        "eu_coffee_2 = np.dot(u_coffee_outcomes, p_coffee_outcomes_coffee) # alternatives\n",
        "print('EU(coffee) = ', eu_coffee)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KRlrp3CUH27",
        "outputId": "2fedf080-18f2-4f03-fe56-33c599cabecc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EU by outcome = [ 2. -9.]\n",
            "EU(shop) =  -7.0\n"
          ]
        }
      ],
      "source": [
        "shop_outcomes = [\"good\", \"waste\"]\n",
        "u_shop_outcomes = np.array([20, -10])\n",
        "p_shop_outcomes_shop = np.array([0.1, 0.9])\n",
        "\n",
        "# The weighted utility ofeach outcome is each to compute by pairwise multiplication\n",
        "eu_shop_outcome = u_shop_outcomes * p_shop_outcomes_shop\n",
        "print('EU by outcome =', eu_shop_outcome)\n",
        "\n",
        "# Summing the weighted utilities gets us the expected utility\n",
        "eu_shop = np.sum(eu_shop_outcome)\n",
        "eu_shop_2 = np.dot(u_shop_outcomes, p_shop_outcomes_shop) # alternatives\n",
        "print('EU(shop) = ', eu_shop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QW7tjdMUJdH",
        "outputId": "e379e617-2e7a-49a4-f878-6a43df1cd116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EU by outcome = [1.6 1. ]\n",
            "EU(eat) =  2.6\n"
          ]
        }
      ],
      "source": [
        "eat_outcomes = [\"hungry\", \"filling\"]\n",
        "u_eat_outcomes = np.array([2, 5])\n",
        "p_eat_outcomes_eat = np.array([0.8, 0.2])\n",
        "\n",
        "# The weighted utility ofeach outcome is each to compute by pairwise multiplication\n",
        "eu_eat_outcome = u_eat_outcomes * p_eat_outcomes_eat\n",
        "print('EU by outcome =', eu_eat_outcome)\n",
        "\n",
        "# Summing the weighted utilities gets us the expected utility\n",
        "eu_eat = np.sum(eu_eat_outcome)\n",
        "eu_eat_2 = np.dot(u_eat_outcomes, p_eat_outcomes_eat) # alternatives\n",
        "print('EU(eat) = ', eu_eat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEuyTyOGUiGA"
      },
      "source": [
        "What should you do if you take the principle of maximum expected utility to be your decision criterion?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvpPv5rGUfNg",
        "outputId": "68eed070-b22d-41f8-cfcc-4b0bd613f5ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coffee is the MEU choice\n"
          ]
        }
      ],
      "source": [
        "if eu_eat > eu_coffee and eu_eat > eu_shop:\n",
        "  print('Eat is the MEU choice')\n",
        "elif eu_coffee > eu_eat and eu_coffee > eu_shop:\n",
        "  print('Coffee is the MEU choice')\n",
        "else:\n",
        "  print('Shop is the MEU choice')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr3PHeRGVVVi"
      },
      "source": [
        "What should you do if you take the principle of maximax decision criterion to be your decision criterion?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNqyW9nbVscS",
        "outputId": "0c02a77e-453b-4808-abaa-7fabe135431b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MaxU(coffee) = 10\n",
            "MaxU(eat) = 5\n",
            "MaxU(shop) = 20\n",
            "Shop is the Maximax choice\n"
          ]
        }
      ],
      "source": [
        "# The utility of each choice is the max utility of their outcomes\n",
        "max_u_coffee = np.max(u_coffee_outcomes)\n",
        "print('MaxU(coffee) =', max_u_coffee)\n",
        "max_u_eat = np.max(u_eat_outcomes)\n",
        "print('MaxU(eat) =', max_u_eat)\n",
        "max_u_shop = np.max(u_shop_outcomes)\n",
        "print('MaxU(shop) =', max_u_shop)\n",
        "\n",
        "\n",
        "# The decision criterion is then to pick the outcome with the highest utility:\n",
        "if max_u_coffee > max_u_eat and max_u_coffee > max_u_shop:\n",
        "  print('Coffee is the Maximax choice')\n",
        "elif max_u_eat > max_u_coffee and max_u_eat > max_u_shop:\n",
        "  print('Eat is the Maximax choice')\n",
        "else:\n",
        "  print('Shop is the Maximax choice')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoE2eyoOWVXp"
      },
      "source": [
        "What should you do if you take the principle of maximin decision criterion to be your decision criterion?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4VDWbjGWXxw",
        "outputId": "d66e1eb4-2b39-48df-926d-88672d978dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MinU(coffee) = -5\n",
            "MinU(eat) = 2\n",
            "MinU(shop) = -10\n",
            "Eat is the Maximin choice\n"
          ]
        }
      ],
      "source": [
        "min_u_coffee = np.min(u_coffee_outcomes)\n",
        "print('MinU(coffee) =', min_u_coffee)\n",
        "min_u_eat = np.min(u_eat_outcomes)\n",
        "print('MinU(eat) =', min_u_eat)\n",
        "min_u_shop = np.min(u_shop_outcomes)\n",
        "print('MinU(shop) =', min_u_shop)\n",
        "\n",
        "\n",
        "# The decision criterion is then to pick the outcome with the highest utility:\n",
        "if min_u_coffee > min_u_eat and min_u_coffee > min_u_shop:\n",
        "  print('Coffee is the Maximin choice')\n",
        "elif min_u_eat > min_u_coffee and min_u_eat > min_u_shop:\n",
        "  print('Eat is the Maximin choice')\n",
        "else:\n",
        "  print('Shop is the Maximin choice')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRLlnsWJ90DZ"
      },
      "source": [
        "## EXERCISE 2: Solving a MDP with MDP toolbox\n",
        "\n",
        "We have four states and four actions.\n",
        "\n",
        "The actions are: 0 is Right, 1 is Left, 2 is Up and 3 is Down.\n",
        "\n",
        "The states are 0, 1, 2, 3, and they are arranged like this:\n",
        "    \n",
        "$$\n",
        "\\begin{array}{cc}\n",
        "2 & 3\\\\\n",
        "0 & 1\\\\\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "The motion model provides:\n",
        "*   0.8 probability of moving in the direction of the action,\n",
        "*   0.1 probability of moving in each of the directions perpendicular to that of the action.\n",
        "\n",
        "So that 2 is Up from 0 and 1 is Right of 0, and so on. The cost of any action (in any state) is -0.04.\n",
        "\n",
        "In case of \"infeasible\" movements, the agent remains in the current state with probability 0.8+0.1. Consider the perpendicular directions in any case.\n",
        "\n",
        "The reward for state 3 is 1, and the reward for state 1 is -1, and the agent does not leave those states.\n",
        "\n",
        "Set discount factor equal to 0.99.\n",
        "\n",
        "> __QUESTION 2(a):__ What is the policy based on the Value iteration algorithm?\n",
        "\n",
        "> __QUESTION 2(b):__ What is the policy based on the Policy iteration algorithm?\n",
        "\n",
        "> __QUESTION 2(c):__ What is the policy based on the Q-Learning algorithm?\n",
        "\n",
        "> __QUESTION 2(d):__ Look at the **setVerbose**() function and the time attribute of the MDP objects in MDPToolbox and use them to compare the number of iterations (hint: see the iter attribute) and the CPU time used to come up with a solution (hint: see the time attribute) in the Value iteration algorithm and Policy iteration algorithm resolutions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mdptoolbox\n",
        "import numpy as np\n",
        "\n",
        "# 4x4x4\n",
        "\n",
        "P1 = np.array([[[0.1, 0.8, 0.1, 0], # right\n",
        "                [0, 1, 0, 0],\n",
        "                [0.1, 0, 0.1, 0.8],\n",
        "                [0, 0 ,0, 1]],\n",
        "               [[0.9, 0, 0.1, 0], # left\n",
        "                [0, 1, 0, 0],\n",
        "                [0.1, 0, 0.9, 0],\n",
        "                [0, 0 ,0, 1]],\n",
        "               [[0.1, 0.1, 0.8, 0], # up\n",
        "                [0, 1, 0, 0],\n",
        "                [0, 0, 0.9, 0.1],\n",
        "                [0, 0 ,0, 1]],\n",
        "               [[0.9, 0.1, 0, 0], # down\n",
        "                [0, 1, 0, 0],\n",
        "                [0.8, 0, 0.1, 0.1],\n",
        "                [0, 0 ,0, 1]]])\n",
        "\n",
        "# 4x4\n",
        "R1 = np.array([[-0.04, -0.04, -0.04, -0.04], # 0\n",
        "               [-1, -1, -1, -1], # 1\n",
        "               [-0.04, -0.04, -0.04, -0.04], # 2\n",
        "               [1, 1, 1, 1]]) # 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Values:\n",
            " (88.23133912867954, -99.9949804281095, 97.54814303782808, 99.9949804281095)\n",
            "Policy:\n",
            " (1, 0, 0, 0)\n"
          ]
        }
      ],
      "source": [
        "mdptoolbox.util.check(P1, R1)\n",
        "\n",
        "# To run value iteration we create a value iteration object, and run it. Note that \n",
        "# discount value is 0.99\n",
        "vi1 = mdptoolbox.mdp.ValueIteration(P1, R1, 0.99)\n",
        "vi1.run() #do the algorithm iteration\n",
        "\n",
        "# We can then display the values (utilities) computed, and look at the policy:\n",
        "print('Values:\\n', vi1.V)\n",
        "print('Policy:\\n', vi1.policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Values:\n",
            " (88.23635870057002, -99.99999999999991, 97.5531626097185, 99.99999999999991)\n",
            "Policy:\n",
            " (1, 0, 0, 0)\n"
          ]
        }
      ],
      "source": [
        "# To run policy iteration we create a policy iteration object, and run it. Note that \n",
        "# discount value is 0.99\n",
        "pi1 = mdptoolbox.mdp.PolicyIteration(P1, R1, 0.99)\n",
        "pi1.run()\n",
        "# We can then display the values (utilities) computed, and look at the policy:\n",
        "print('Values:\\n', pi1.V)\n",
        "print('Policy:\\n', pi1.policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Values:\n",
            " (3.111743375544036, -12.92743881068282, 24.270626720133063, 70.25515807563649)\n",
            "Policy:\n",
            " (1, 3, 0, 2)\n"
          ]
        }
      ],
      "source": [
        "# To run q-learning we create a q-learning object, and run it. Note that \n",
        "# discount value is 0.99\n",
        "ql1 = mdptoolbox.mdp.QLearning(P1, R1, 0.99)\n",
        "ql1.run()\n",
        "# We can then display the values (utilities) computed, and look at the policy:\n",
        "print('Values:\\n', ql1.V)\n",
        "print('Policy:\\n', ql1.policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look at the **setVerbose**() function and the time attribute of the MDP objects in MDPToolbox and use them to compare the number of iterations (hint: see the iter attribute) and the CPU time used to come up with a solution (hint: see the time attribute) in the Value iteration algorithm and Policy iteration algorithm resolutions."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
