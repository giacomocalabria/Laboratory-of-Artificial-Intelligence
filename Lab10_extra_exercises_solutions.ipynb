{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## EXERCISE 1: What to do at the airport?\n",
        "\n",
        "You are travelling and have some time to kill at the aiport. There are three things you could spend your time doing:\n",
        "  \n",
        "1) You could have a coffee.\n",
        "\n",
        "This has a probability of $0.8$ of giving you time to relax with a tasty beverage, and a utility of $10$.\n",
        "It also has a probability of $0.2$ of providing you with a nasty cup from over-roasted beans that annoys you,\n",
        "and outcome with a utility of $-5$.\n",
        "\n",
        "2) You could shop for clothes.\n",
        "\n",
        "This has a probability of $0.1$ that you will find a great outfit at a good price, utility $20$. However, it\n",
        "has a probability of $0.9$ that you end up wasting money on over-priced junk, utility $-10$.\n",
        "\n",
        "3) You could have a bite to eat.\n",
        "\n",
        "This has a probability of $0.8$ that you find something rather mediocre that prevents you from being too hungry\n",
        "during your flight, utility $2$, and a probability of $0.2$ that you find something filling and tasty, utility $5$.\n",
        "\n",
        "> __QUESTION 1(a):__ What should you do if you take the principle of maximum expected utility to be your decision criterion?\n",
        "\n",
        "> __QUESTION 1(b):__ What should you do if you take the principle of maximax decision criterion to be your decision criterion?\n",
        "\n",
        "> __QUESTION 1(c):__ What should you do if you take the principle of maximin decision criterion to be your decision criterion?\n",
        "    "
      ],
      "metadata": {
        "id": "twbLWgpO7YQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# define the arrays with the probabilities and the utilities from the description of the exercise\n",
        "p_coffee = np.array([0.8,0.2])\n",
        "u_coffee = np.array([10,-5])\n",
        "\n",
        "p_shop = np.array([0.1,0.9])\n",
        "u_shop = np.array([20,-10])\n",
        "\n",
        "p_eat = np.array([0.8,0.2])\n",
        "u_eat = np.array([2,5])\n",
        "\n",
        "#EXPECTED UTILITIES:\n",
        "eu_coffee = np.sum(p_coffee * u_coffee)\n",
        "print(\"Expected utility when getting coffee: \", eu_coffee)\n",
        "\n",
        "eu_shop = np.sum(p_shop * u_shop)\n",
        "print(\"Expected utility when going to shop: \", eu_shop)\n",
        "\n",
        "eu_eat = np.sum(p_eat * u_eat)\n",
        "print(\"Expected utility when going to eat: \", eu_eat)\n",
        "\n",
        "if(eu_coffee > eu_shop):\n",
        "  if(eu_coffee > eu_eat):\n",
        "    print(\"Best MEU choice is to get coffee\")\n",
        "  else:\n",
        "    print(\"Best MEU choice is to go to eat\")\n",
        "else:\n",
        "  if(eu_shop > eu_eat):\n",
        "    print(\"Best MEU choice is to go to shop\")\n",
        "  else:\n",
        "    print(\"Best MEU choice is to go to eat\")\n",
        "\n",
        "#MAXIMAX\n",
        "max_u_coffee= np.max(u_coffee)\n",
        "max_u_shop = np.max(u_shop)\n",
        "max_u_eat = np.max(u_eat)\n",
        "\n",
        "if(max_u_coffee > max_u_shop):\n",
        "  if(max_u_coffee > max_u_eat):\n",
        "    print(\"Best Maximax choice is to get coffee\")\n",
        "  else:\n",
        "    print(\"Best Maximax choice is to go to eat\")\n",
        "else:\n",
        "  if(max_u_shop > max_u_eat):\n",
        "    print(\"Best Maximax choice is to go to shop\")\n",
        "  else:\n",
        "    print(\"Best Maximax choice is to go to eat\")\n",
        "\n",
        "#MAXIMIN\n",
        "min_u_coffee= np.min(u_coffee)\n",
        "min_u_shop = np.min(u_shop)\n",
        "min_u_eat = np.min(u_eat)\n",
        "\n",
        "if(min_u_coffee > min_u_shop):\n",
        "  if(min_u_coffee > min_u_eat):\n",
        "    print(\"Best Maximin choice is to get coffee\")\n",
        "  else:\n",
        "    print(\"Best Maximin choice is to go to eat\")\n",
        "else:\n",
        "  if(min_u_shop > min_u_eat):\n",
        "    print(\"Best Maximin choice is to go to shop\")\n",
        "  else:\n",
        "    print(\"Best Maximin choice is to go to eat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT2EIlT80I9z",
        "outputId": "7d9082a2-7982-456c-f13e-fd9ef81399db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected utility when getting coffee:  7.0\n",
            "Expected utility when going to shop:  -7.0\n",
            "Expected utility when going to eat:  2.6\n",
            "Best MEU choice is to get coffee\n",
            "Best Maximax choice is to go to shop\n",
            "Best Maximin choice is to go to eat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXERCISE 2: Solving a MDP with MDP toolbox\n",
        "\n",
        "We have four states and four actions.\n",
        "\n",
        "The actions are: 0 is Right, 1 is Left, 2 is Up and 3 is Down.\n",
        "\n",
        "The states are 0, 1, 2, 3, and they are arranged like this:\n",
        "    \n",
        "$$\n",
        "\\begin{array}{cc}\n",
        "2 & 3\\\\\n",
        "0 & 1\\\\\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "The motion model provides:\n",
        "*   0.8 probability of moving in the direction of the action,\n",
        "*   0.1 probability of moving in each of the directions perpendicular to that of the action.\n",
        "\n",
        "So that 2 is Up from 0 and 1 is Right of 0, and so on. The cost of any action (in any state) is -0.04.\n",
        "\n",
        "In case of \"infeasible\" movements, the agent remains in the current state with probability 0.8+0.1. Consider the perpendicular directions in any case.\n",
        "\n",
        "The reward for state 3 is 1, and the reward for state 1 is -1, and the agent does not leave those states.\n",
        "\n",
        "Set discount factor equal to 0.99.\n",
        "\n",
        "> __QUESTION 2(a):__ What is the policy based on the Value iteration algorithm?\n",
        "\n",
        "> __QUESTION 2(b):__ What is the policy based on the Policy iteration algorithm?\n",
        "\n",
        "> __QUESTION 2(c):__ What is the policy based on the Q-Learning algorithm?\n",
        "\n",
        "> __QUESTION 2(d):__ Look at the **setVerbose**() function and the time attribute of the MDP objects in MDPToolbox and use them to compare the number of iterations (hint: see the iter attribute) and the CPU time used to come up with a solution (hint: see the time attribute) in the Value iteration algorithm and Policy iteration algorithm resolutions."
      ],
      "metadata": {
        "id": "wRLlnsWJ90DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the libraries\n",
        "!pip install pymdptoolbox\n",
        "import numpy as np\n",
        "import mdptoolbox as mt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG6ykAt-3UbR",
        "outputId": "8f2a8e40-c899-4d20-8c94-b0eeaa541fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymdptoolbox in /usr/local/lib/python3.10/dist-packages (4.0b3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since the probability array is (A, S, S), there are 4 arrays. Each\n",
        "# is a 4 x 4 array:\n",
        "# Since the probability array is (A, S, S), there are 4 arrays. Each\n",
        "# is a 4 x 4 array.\n",
        "#NB: State 1 and State 3 are absorbing. The agent does not leave those states.\n",
        "transitions = np.array([[[0.1, 0.8, 0.1, 0  ], # Right, State 0, the agent can remain in State 0 (p=0.1), it can move from state 0 to state 1 with a right action (p=0.8) or it can move to state 2 (p=0.1) -> perpendicular direction\n",
        "                [0,   1,   0,   0  ], # State 1 is absorbing\n",
        "                [0.1, 0,   0.1, 0.8],\n",
        "                [0,   0,   0,   1  ]],# State 3 is absorbing\n",
        "               [[0.9, 0,   0.1, 0  ], # Left: it is infeasible moving left so the agent stays in State 0 (p=0.8+0.1), but it can move up (p=0.1) since it is a perpendicular direction\n",
        "                [0,   1,   0,   0  ], # State 1 is absorbing\n",
        "                [0.1, 0,   0.9, 0  ],\n",
        "                [0,   0,   0,   1  ]], # State 3 is absorbing\n",
        "               [[0.1, 0.1, 0.8, 0  ], # Up\n",
        "                [0,   1,   0,   0  ], # State 1 is absorbing\n",
        "                [0,   0,   0.9, 0.1],\n",
        "                [0,   0,   0,   1  ]], # State 3 is absorbing\n",
        "               [[0.9, 0.1, 0,   0  ], # Down\n",
        "                [0,   1,   0,   0  ], # State 1 is absorbing\n",
        "                [0.8, 0,   0.1, 0.1],\n",
        "                [0,   0,   0,   1  ]]]) # State 3 is absorbing\n",
        "\n",
        "# The reward array has one set of values for each state. Each is the\n",
        "# value of all the actions. Here there are four actions, all with the\n",
        "# usual cost:\n",
        "rewards = np.array([[-0.04, -0.04, -0.04, -0.04],\n",
        "                    [-1,    -1,    -1,    -1],\n",
        "                    [-0.04, -0.04, -0.04, -0.04],\n",
        "                    [1,      1,     1,     1]])\n",
        "\n",
        "discount_factor = 0.99\n",
        "\n",
        "\n",
        "vi = mt.mdp.ValueIteration(transitions, rewards, discount_factor)\n",
        "#vi.setVerbose()\n",
        "vi.run()\n",
        "print(\"Value Iteration policy:\", vi.policy)\n",
        "print(\"Value Iteration iterations:\", vi.iter)\n",
        "print(\"Value Iteration CPU time:\", vi.time)\n",
        "\n",
        "\n",
        "print(\"------------------------------------------\")\n",
        "pi = mt.mdp.PolicyIteration(transitions, rewards, discount_factor)\n",
        "#pi.setVerbose()\n",
        "pi.run()\n",
        "print(\"Policy Iteration policy:\", pi.policy)\n",
        "print(\"Policy Iteration iterations:\", pi.iter)\n",
        "print(\"Policy Iteration CPU time:\", pi.time)\n",
        "\n",
        "\n",
        "print(\"------------------------------------------\")\n",
        "\n",
        "ql = mt.mdp.QLearning(transitions, rewards, discount_factor)\n",
        "#ql.setVerbose()\n",
        "ql.run()\n",
        "print(\"Q-Learning policy:\", ql.policy)\n",
        "#QL Object has not iter and time attributes"
      ],
      "metadata": {
        "id": "TwpUpFn53prI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07de0ccc-0958-45c5-e60b-913385a53ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Iteration policy: (1, 0, 0, 0)\n",
            "Value Iteration iterations: 985\n",
            "Value Iteration CPU time: 0.028118133544921875\n",
            "------------------------------------------\n",
            "Policy Iteration policy: (1, 0, 0, 0)\n",
            "Policy Iteration iterations: 3\n",
            "Policy Iteration CPU time: 0.0015273094177246094\n",
            "------------------------------------------\n",
            "Q-Learning policy: (1, 3, 0, 1)\n"
          ]
        }
      ]
    }
  ]
}